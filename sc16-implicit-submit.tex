\documentclass[conference]{IEEEtran}

% Fix to revert the changes IEEEtran makes to table caption styles
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatother
\usepackage[font=scriptsize,justification=justified,singlelinecheck=false]{caption}

%===========================================================================
\usepackage{listings}
\lstloadlanguages{C++,Pascal}

% Settings for the lstlistings environment
\lstset{
language=C++,                       % choose the language of the code
basicstyle=\footnotesize\ttfamily,  % the size of the fonts that are used for the
                                    % code
numbers=none,                       % where to put the line-numbers
numberstyle=\tiny,                  % the size of the fonts that are used for the
                                    % line-numbers
stepnumber=1,                       % the step between two line-numbers. If it's
                                    % 1 each line will be numbered
numbersep=5pt,                      % how far the line-numbers are from the code
%backgroundcolor=\color{gray},      % choose the background color. You must add
                                    % \usepackage{color}
showspaces=false,                   % show spaces adding particular underscores
showstringspaces=false,             % underline spaces within strings
showtabs=false,                     % show tabs within strings adding particular
                                    % underscores
keywordstyle=\bfseries\color{blue},  % color of the keywords
commentstyle=\color{darkgreen},     % color of the comments
stringstyle=\color{darkred},        % color of strings
captionpos=b,                       % sets the caption-position to top
tabsize=2,                          % sets default tabsize to 2 spaces
frame=tb,                           % adds a frame around the code
breaklines=true,                    % sets automatic line breaking
breakatwhitespace=false,            % sets if automatic breaks should only happen
                                    % at whitespace
escapechar=\%,                      % toggles between regular LaTeX and listing
belowskip=0.3cm,                    % vspace after listing
morecomment=[s][\bfseries\color{blue}]{struct}{\ },
morecomment=[s][\bfseries\color{blue}]{class}{\ },
morecomment=[s][\bfseries\color{blue}]{public:}{\ },
morecomment=[s][\bfseries\color{blue}]{public}{\ },
morecomment=[s][\bfseries\color{blue}]{protected:}{\ },
morecomment=[s][\bfseries\color{blue}]{private:}{\ },
morecomment=[s][\bfseries\color{black}]{operator+}{\ },
xleftmargin=0.1cm,
%xrightmargin=0.1cm,
}

\usepackage{color}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{cite}
\newcommand{\fix}[1]{{\bf \textcolor {red}{#1}}}
%===========================================================================
\begin{document}
\title{Solving Large Quantities of Small Matrix Problems on Cache-Coherent SIMD Architectures}

%===========================================================================
% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{
\IEEEauthorblockN{Bryce Lelbach, Hans Johansen, and Samuel Williams}
\IEEEauthorblockA{Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA 94720\\ {\it \{balelbach, hjohansen, swwilliams\}@lbl.gov}}
%\and
%\IEEEauthorblockN{Homer Simpson}
%\IEEEauthorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle

%===========================================================================
\begin{abstract}
Horizontal Explicit/Vertical Implicit (HE/VI) methods show promise as a
scalable approach to solving global climate problems on cubed sphere
geometries. When solving problems with a high horizontal-vertical aspect ratio
HE/VI methods, it is necessary to perform a large number of small vertical
implicit solves which are independent of each other. We present a performant
approach for solving large quantities of independent matrix problems on
cache-coherent SIMD architectures. 
\end{abstract}

% no keywords


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

%===========================================================================
Submission info:
\url{https://easychair.org/conferences/?conf=pmbs16}

\section*{Outline}
\begin{itemize}
\item Climate apps use HE-VI model 3D+1D. Also a pattern for 2D+1D and 3D+0D chemistry
\item Results demonstrate importance of ``batching'' solves, MLK doesn't do
\item Cache coherency is important: IMEX RK accum, tiling
\item Explicit part is HO stencil op, memory b/w bound (w/ ghost cells)
\item Implicit part is non-linear solver: App requires different matrix at each i,j index 
\item Results in repeated vertical sparse banded solve
\item Solve (tridiag, banded, dense) should vectorize on i (unit stride), tiled in pencils
\item Considerations for vector alignment, tiling, and memory
\item Performance results, scaling, comparison, by platform / parameter
\item Future work: communication hiding, load imbalance, IMEX
\end{itemize}
(Bryce's email comments) Looking over the outline:
\\
It's crucial that we clearly identify the optimizations that we
believe are novel/high-impact, vs the optimizations that are
well-known to the community (even if they were not well known to us).
For example, on the outline, we have listed "IMEX RK accum, tiling".
Do we want to present the accumulation optimizations which removed
unnecessary temporaries from the time integrator? Do we feel that
optimization is novel, or is that just a common-sense thing that only
affected us because of the Chombo programming model? Likewise, do we
want to present tiling as a focal point in this paper? Tiling is a
well-known technique; we certainly can't claim tiling in general as
novel work. Is some part of our tiling approach novel? (how about
parallelizing the tile loop - is that fairly novel? I know Sam does
this in HPGMG, but is this a widely used technique?)
\\
We would be well served by applying the scientific method at this juncture:
\\
\begin{itemize}
\item Question: What concrete research question(s) are we answering?
\begin{itemize}
\item Prior research indicates that HE-VI methods show promise as a
scalable approach to solving global climate problems on cubed sphere
geometries because <explanation> [cite prior studies]. How do we
implement HE-VI methods which perform well on cache-coherent SIMD
architectures?
\item When solving a problem with a high horizontal-vertical aspect ratio
(e.g. the horizontal extents are much greater than the vertical
extents) with HE-VI methods, it is necessary to perform a large number
of small vertical implicit solves (<give examples of matrix sizes>
[citation]) which are independent of each other. <explain the type of
solves in the climate dycore - e.g. non-linear but nearly banded>
[citation]. How can we efficiently solve large quantities of small
non-linear bandedish/banded/tridiagonal matrices on cache-coherent
SIMD architectures?
\end{itemize}
\item Hypothesis: What theories did we come up with that would answer our
research question(s)?
\begin{itemize}
\item Optimal memory access patterns, management of working set sizes
(e.g. staying in cache) and efficient use of vector units are
necessary to achieve good performance on cache-coherent SIMD
architectures.
\begin{itemize}
\item Optimal memory access patterns: moving through memory in unit
stride, controlling the number of streams.
\item Management of working set sizes: tiling, reducing size of
temporaries/localizing temporaries (thread-local or otherwise)
\item Efficient use of vector units: moving through memory in unit
stride, controlling memory alignment, controlling array strides,
annotation-assisted autovectorization
\item TODO: List of individual, concrete optimizations from above.
\end{itemize}
\item Mainstream linear algebra libraries (<examples> [citation]) are not
well-suited for solving large quantities of small non-linear
bandedish/banded/tridiagonal matrices on cache-coherent SIMD
architectures because <expalanation> [cite prior studies if possible].
\end{itemize}


\item Prediction: If our hypotheses are true, what results can we expect to
see? \\
$\rightarrow$ TODO: What performance characteristics should we see with and
without each of the concrete optimizations from our hypothesis above?

\item Experiment: Investigate the predictions we've made. \\
$\rightarrow$ TODO: Methodology for benchmarking the optimizations identified
above and measuring the performance characteristics we're interested
in.

\item Analysis: What were the results of our experiments?
\end{itemize}

List of figures from meeting on 8/11:
For KNL, HSW, SNB(?)...
\begin{enumerate}
\item Baseline performance using MKL (and hand) using i-major data layout (not
vectorized)
\item Performance as a function of 32b RCP NR (not needed on KNL), stored
reciprocal (cuts divides in half) for fixed file size (4?)
\item Performance vs. Tile Size (jtile = 1,2,4,8,16,32)
\item Best Performance vs. MKL using same data layout vs. MKL using its best
(include DRAM BW limit)
\item Performance as a function of total parallelism for constant tile size
[optional time permitting]
\item Performance with 2,3,4 hyperthreads...  maybe just prose comments?
\item effect of kdim != pow(2)... maybe just prose comments?
\end{enumerate}

% 

%===========================================================================
\section{Introduction}
% 
\fix{Hans to write}
One important class of problems in computational science is the solving
  of smaller-dimensional matrix subproblems that are duplicated across
  many degrees of freedom in a larger two or more dimension computation.
Several examples of this include: 
\begin{itemize}
\item Pointwise chemistry systems in the context of a larger, 
  flow simulations. 
Examples include geochemistry \cite{??}, cloud microphysics \cite{??},
  and combustion \cite{??};
\item Solving one-dimensional systems that represent a ``primary''
  direction for a physical phenomenon.
Examples here include atmospheric radiation \cite{??}, groundwater
  penetration \cite{??}, or models for cloud convection \cite{??}; and
\item Implicit solvers that need to couple these kinds of subsystems, 
  such as physics-based preconditioners \cite{??}, alternating 
  direction implicit ADI, \cite{??}, and operator-split or
  semi-implicit time integrators \cite{??}.
\end{itemize}
In most cases, these matrices are relatively small 
  (ranging from $O(10-100)$ chemistry components or ``levels'' 
  in a climate application), and may be sparse or dense, but must 
  to be solved repeatedly, but with different entries each time, 
  to advance the overall simulation.
Thus, because these are often non-linear matrix systems with space- and 
  time-dependent entries, these applications may not use a 
  ``factor once, solve many times'' approach, which is often used 
  as a model matrix performance test.
This prevents amortizing setup and factorization costs
  across multiple right-hand side solves as in \emph{dgxxxxx} \cite{??},
  and also challenges SIMD vectorization due to the odd size and
  dissimilar entries of the matrices, as well as the 
  memory access patterns relative to the bigger simulation data layout.
In that case, it is usually sub-optimal on many-core SIMD 
  or SIMT GPU architectures,
  to just call an optimized linear algebra library, 
  such as Intel's MKL version of LAPACK \cite{??} or NVIDIA's
  cuBLAS \cite{??}; these may not achieve peak memory bandwidth 
  and VPU performance across the range of small matrix size. 
In that sense, it can leads applications to create their own 
  custom implementations, which may not be optimal, and create a 
  (potentially unnecessary) maintainance burden for the applications 
  running across multiple many-core architectures as well. 
  
To this end, we have developed a model matrix kernel that mimics what
  is encountered in these kinds of large-scale simulations.
Key aspects of the code include:
\begin{itemize}
\item Matrix systems that must be created and solved
  at each point in a two-dimensional subdomain (represented by $(i,j)$ indices)
  of a three-dimensional application (that is, $(i,j,k)$ indices).
\item Each matrix is tri-diagonal, and must be solved for all $O(30-100)$
  values in the $k$ index.
\item The matrix is derived from finite difference discretization for the
  1D diffusion equation, which allows it to be solved without pivoting
  (pivoting will be addressed in future work).
\end{itemize}

%===========================================================================
\section{Related Work}
\fix{Hans to write - put in }
Is this unique? Have to look at batched / tiled sparse solvers, on Intel
architecture.

MKL

Links Hans sent
\begin{itemize}
\item Built-to-order BLAS \cite{Spampinato:2014}
\item Blaze \cite{BlazeSite}
\item PLASMA \cite{PLASMASite}
\item MAGMA \cite{Haidar:2015}
\item libxsmm \cite{??}
\end{itemize}


etc...

%===========================================================================
\section{Implementation}
\fix{Bryce to write}

To solve a positive-definite symmetric tridiagonal system \fix{diagonally
dominant?}, a simplified form of Gaussian elimination which does not perform
any pivoting can be used. This method is known as the Thomas algorithm~\cite{},
and it is \(O(n)\) in time, a significant improvement over full Gaussian
elimination, which is \(O(n^3)\) in time.

\[
\begin{bmatrix}
b_0 & c_0 &     &         & 0       \\
a_1 & b_1 & c_1 &         &         \\
    & a_2 & b_2 & ...     &         \\
    &     & ... & ...     & c_{n-2} \\
0   &     &     & a_{n-1} & b_{n-1}
\end{bmatrix}
\begin{bmatrix}
u_0     \\
u_1     \\
...     \\
...     \\
u_{n-1}
\end{bmatrix}
=
\begin{bmatrix}
u_0     \\
u_1     \\
...     \\
...     \\
u_{n-1}
\end{bmatrix}
\]

Forward elimination:

\begin{lstlisting}
for (index_type k = 1; k < n; ++k) {
    double const m = a[k] / b[k - 1];
    b[k] = b[k] - m * c[k - 1];
    u[k] = u[k] - m * u[k - 1];
} 
\end{lstlisting}

Backward substitution:

\begin{lstlisting}
u[n - 1] = d[n - 1] / b[n - 1];

for (index_type k = n - 2 ; k >= 0; --k) {
    u[k] = (u[k] - c[k] * u[k + 1]) / b[k];
} 
\end{lstlisting}

TODO: Mention that algorithm is in-place and destroys the input matrix. Describe
AI.

In the applications described in Section~\ref{}, we perform such a solve on
each vertical (e.g. \(z\)) "column" in a 3D Cartesian grid (Figure~\ref{1}). The
matrix coefficients for each column depend on the problem state, so a unique
matrix for each column needs to be constructed before each solve. There are two
different approaches to computing these batch solves:

\begin{itemize}
\item \textbf{Solve Columns Independently}: The most straightforward approach is to deal with each tridiagonal solve separately, independent of the other solves. An \(nz\)x\(nz\) matrix is constructed for each column, and then the Thomas algorithm is used to solve the system formed by the matrix and the vertical column. Because the column solves are independent, different column solves can be executed concurrently via task-level parallelism. Additionally, vectorization can be attempted in the vertical (\(z\)) dimension for each independent solve. 
\item \textbf{Interleave Column Solves}: The alternative approach is to simultaneously solve multiple columns. For each \(tw_x\)x\(tw_y\) horizontal tile of the grid, a \(tw_x\)x\(tw_y\) subgrid of \(nz\)x\(nz\) matrices (e.g. a 4D grid stored in a contiguous memory region) is constructed, and then a modified Thomas algorithm is used to simultaneously solve \(tw_x\)x\(tw_y\) independent vertical columns. Each tile is solved independently, so the tile solves can be executed concurrently via task-level parallelism. Data-level parallelism can be applied in one of the two horizontal dimensions (\(x\) or \(y\)).
\end{itemize}

The interleaved approach offers a major benefit over the independent approach;
it allows vectorization in one of the horizontal dimensions. The independent
approach is restricted to vectorizing in the vertical dimension. The extent of
the vertical dimension (\(nz\)) tends to be very small in the applications we are
concerned with (\fix{HOW SMALL}). In this approach, the loops we are trying to
vectorize will typically have iteration counts between 10 and 100. If we
vectorize in one of the horizontal dimensions, we can control the iteration
count of the loops via the tile size. Additionally, we cannot vectorize the
forward-elimination loop, because it contains a read-after-write dependency
(\fix{ELABORATE}).

Thus, we pick one of the horizontal dimensions as our unit stride dimension.
The vertical dimension will have the greatest stride. 

%===========================================================================
\section{Experimental Setup}
\fix{Bryce to write}
Edison~\cite{Edison_website}, Cori, etc...
Compilers
Problem Size

\begin{itemize}
\item 3x3 or 30x30 or 400x400
\item Vectorized vs. MKL / non-vectorized
\item Mixed precision
\item Tiled (experiment for non-tiled code blowing out cache)
\end{itemize}

%===========================================================================
\section{Results and Analysis}
\fix{Bryce to write}

\begin{table}%[htbp]
\center
\scriptsize
\caption{\textbf{Comparison of Mixed-Precision and Double-Precision Algorithms}:
To study the trade-off in performance and accuracy between the mixed-precision
and double-precision variants of our algorithm, we solved a 1D diffusion
problem with both codes on a single Intel Xeon ??? "Ivy Bridge" core. We used
two measures to quantify error: the L2 norm of the difference between the
analytic solution and the computing solution, and the absolute maximum of the
residual vector of the tridiagonal solve. The test problem had storage
requirements of approximately ~4.3 GB. 10 sample runs were performed with each
sample performing 5 time steps. The walltime and uncertainty metrics below are
normalized to the double-precision results; the L2 norm and residual metrics
specify order of magnitude.
}
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{|c|c|c|c|} \hline
\textbf{Algorithm} & \textbf{Walltime (Normalized)} & \textbf{L2 Norm} & \textbf{Residual} \\ \hline
Double-Precision   & 1.0   $\pm$ 0.007              & O(1e-07)         & O(1e-16)          \\ \hline 
Mixed-Precision    & 0.836 $\pm$ 0.006              & O(1e-06)         & O(1e-08)          \\ \hline
\end{tabular}
\label{tab:mixed_vs_double}
\end{table}


%===========================================================================
\section{Conclusion}
\fix{Bryce to write}
The conclusion goes here.


%===========================================================================
\section*{Acknowledgments}
\fix{Comment this section out before submitting... Reformat before finalizing...}
\fix{Intel IPCC ack, too}

This research used resources in Lawrence Berkeley National Laboratory and the National Energy Research Scientific Computing Center, which are supported by the U.S. Department of Energy Office of Science's Advanced Scientific Computing Research program under contract number DE-AC02-05CH11231.  
This material is based upon work supported by the U.S. Department of Energy, Office of Science, Advanced Scientific Computing Research, Scientific Discovery through Advanced Computing (SciDAC) program.
%This research used resources of the National Energy Research Scientific Computing Center (NERSC), which is supported by the Office of Science of the U.S. Department of Energy under contract DE-AC02-05CH11231.
%This research used resources of the Argonne Leadership Computing Facility at Argonne National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under contract DE-AC02-06CH11357.
%This research used resources of the Oak Ridge Leadership Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.


%===========================================================================
\bibliographystyle{IEEEtran}
\bibliography{sc16-implicit}
%===========================================================================

\end{document}


